{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b76d42b5-c66e-4fa8-8cb6-76a29e2764a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from datasets) (2.2.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-20.0.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.11.18-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.6.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.4.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (72 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading aiohttp-3.11.18-cp313-cp313-macosx_11_0_arm64.whl (454 kB)\n",
      "Downloading multidict-6.4.3-cp313-cp313-macosx_11_0_arm64.whl (36 kB)\n",
      "Downloading yarl-1.20.0-cp313-cp313-macosx_11_0_arm64.whl (94 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.6.0-cp313-cp313-macosx_11_0_arm64.whl (120 kB)\n",
      "Downloading propcache-0.3.1-cp313-cp313-macosx_11_0_arm64.whl (44 kB)\n",
      "Downloading pyarrow-20.0.0-cp313-cp313-macosx_12_0_arm64.whl (30.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp313-cp313-macosx_11_0_arm64.whl (30 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/12\u001b[0m [datasets]/12\u001b[0m [datasets]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 datasets-3.6.0 dill-0.3.8 frozenlist-1.6.0 multidict-6.4.3 multiprocess-0.70.16 propcache-0.3.1 pyarrow-20.0.0 xxhash-3.5.0 yarl-1.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1d43fbe-77e0-4389-81bf-c370c3e0e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94a24724-f31a-4d2a-bd4e-1408c2de24f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus total : 28900 phrases (avec augmentation)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2adf2ff4d2854e23b2ec43baf535c012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e5793c66844cb8bc64ba160bf0649e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/508 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab207b1b8294eb99e8b9239ad65e6ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/811k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176d65c3241d495d85654895c6faa8dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.40M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f947f156e2468fa50047a0981f6714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prêt pour le fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "############@@ Lecture Corpus ##############@\n",
    "\n",
    "def parse_conllu(filepath):\n",
    "    sentences = []\n",
    "    current = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                if current:\n",
    "                    sentences.append(current)\n",
    "                    current = []\n",
    "                continue\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) >= 4 and '.' not in parts[0] and '-' not in parts[0]:\n",
    "                word = parts[1]\n",
    "                upos = parts[3]\n",
    "                current.append((word, upos))\n",
    "    if current:\n",
    "        sentences.append(current)\n",
    "    return sentences\n",
    "\n",
    "######### swap de mots ayant le même tag ################\n",
    "\n",
    "def build_pos_dict(corpus):\n",
    "    pos_dict = defaultdict(list)\n",
    "    for sent in corpus:\n",
    "        for word, tag in sent:\n",
    "            if len(word) > 2:\n",
    "                pos_dict[tag].append(word)\n",
    "    return pos_dict\n",
    "\n",
    "def augment_sentence(sentence, pos_dict, p=0.3):\n",
    "    new_sentence = []\n",
    "    for word, tag in sentence:\n",
    "        if tag in pos_dict and random.random() < p:\n",
    "            substitutes = [w for w in pos_dict[tag] if w != word]\n",
    "            if substitutes:\n",
    "                word = random.choice(substitutes)\n",
    "        new_sentence.append((word, tag))\n",
    "    return new_sentence\n",
    "\n",
    "def augment_corpus(corpus, pos_dict, n_augments=2):\n",
    "    augmented = []\n",
    "    for sentence in corpus:\n",
    "        for _ in range(n_augments):\n",
    "            augmented.append(augment_sentence(sentence, pos_dict))\n",
    "    return augmented\n",
    "\n",
    "################ Transformer en Dataset Hugging Face ################\n",
    "\n",
    "def prepare_dataset(sentences):\n",
    "    data = {\n",
    "        \"tokens\": [[word for word, _ in sent] for sent in sentences],\n",
    "        \"labels\": [[tag for _, tag in sent] for sent in sentences]\n",
    "    }\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "\n",
    "#############@ Encodage avec Tokenizer ##############\n",
    "\n",
    "def encode_dataset(dataset, tokenizer, label2id):\n",
    "    def tokenize_and_align(example):\n",
    "        tokenized = tokenizer(example[\"tokens\"], is_split_into_words=True, truncation=True, padding=\"max_length\")\n",
    "        labels = []\n",
    "        for i, word_idx in enumerate(tokenized.word_ids()):\n",
    "            if word_idx is None:\n",
    "                labels.append(-100)\n",
    "            else:\n",
    "                labels.append(label2id[example[\"labels\"][word_idx]])\n",
    "        tokenized[\"labels\"] = labels\n",
    "        return tokenized\n",
    "\n",
    "    return dataset.map(tokenize_and_align, batched=False)\n",
    "    \n",
    "\n",
    "############# Pipeline ##############\n",
    "\n",
    "def main():\n",
    "    # Paramètres\n",
    "    conllu_path = \"../data/UD_French-GSD/UD_French-GSD-master/fr_gsd-ud-train.conllu\"\n",
    "    pretrained_model = \"camembert-base\"\n",
    "\n",
    "    # Chargement\n",
    "    base_corpus = parse_conllu(conllu_path)\n",
    "    pos_dict = build_pos_dict(base_corpus)\n",
    "\n",
    "    # Augmentation\n",
    "    augmented = augment_corpus(base_corpus, pos_dict, n_augments=1)\n",
    "    full_data = base_corpus + augmented\n",
    "    print(f\"Corpus total : {len(full_data)} phrases (avec augmentation)\")\n",
    "\n",
    "    # Créer Dataset Hugging Face\n",
    "    dataset = prepare_dataset(full_data)\n",
    "\n",
    "    # Créer les mappings label2id\n",
    "    unique_labels = sorted({label for ex in dataset[\"labels\"] for label in ex})\n",
    "    label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "    # Tokenizer & encodage\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "    encoded = encode_dataset(dataset, tokenizer, label2id)\n",
    "\n",
    "    # Division train/test\n",
    "    dataset_dict = DatasetDict({\n",
    "        \"train\": encoded.train_test_split(test_size=0.2)[\"train\"],\n",
    "        \"test\": encoded.train_test_split(test_size=0.2)[\"test\"]\n",
    "    })\n",
    "\n",
    "    print(\"Dataset prêt pour le fine-tuning.\")\n",
    "    return dataset_dict, label2id, id2label\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_dict, label2id, id2label = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e17180-ec4d-43c7-b18e-219af3dd1355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
